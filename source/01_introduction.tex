\section{Introduction}

A longstanding goal of the cognitive sciences is to understand cognition, behavior, and experience as it unfolds in the natural world \citep{Parada2020-qq}. Given the technological advancements made in the last decade, there are few methodological roadblocks to understanding natural cognition where laboratory studies can be extended to naturalistic settings and hopefully lead towards new insights \citep{Ladouce2016-li, Parada2018-vf}. More recently, a pragmatic turn has emerged in the field where there is a greater push towards incorporating the body and bodily actions to infer cognitive function \citep{Engel2013-bx}. 

Human tool use is an explicitly natural cognitive function that involves the transfer of proximal goals (e.g., placement of grasp) to distal goals for the tool \citep{Arbib2009-wa}.  Moreover, simple tools fundamentally expand the body representations to include representations of the tool in the peripersonal space \citep{Berti2000-ap, Farne2005-ns, Maravita2002-sq}. Furthermore, tool use is differentiated from other object-based actions where the tool is “acted with” \citep{Johnson2003-ee} and requires semantic knowledge of the tool as well as the necessary skill to perform actions with it \citep{Johnson-Frey2004-qk}. Hence, tool use involves complex behaviors ranging from cognitive and semantic reasoning to perceptual and motor processing.

When using tools, a wealth of information is parsed to produce the relevant action. The semantic knowledge associated with the tool helps understand how it is used, the mechanical knowledge maps the physical properties of the tool for potential usage, and finally, sensorimotor knowledge helps decipher possible movements required to use the tool \citep{Baumard2014-kv}. The amalgamation of these knowledge sources (which can be unique to a tool) necessitates planning any action associated with the tool. When this knowledge is not readily available, inferential processes must be deployed to deduce the relevant action. 

In naturalistic settings, studies have shown that eye movements are made to locations in the scene in anticipation of the following action \citep{Hayhoe2004-ni, Land2001-do, Pelz2001-cn}. \citet{Belardinelli2016-kf}  showed that eye movements are goal-oriented and are modulated in anticipation of the object interaction task. There is strong evidence that task plays a vital role in how the eyes scan the scene and are differentiated between passive viewing and pantomimed interaction \citep{Belardinelli2015-in}. Similarly, \citet{Keshava2020-cp} showed that rudimentary object interactions can be decoded using eye-movement data alone. rudimentary object interactions can be decoded using eye-movement data alone. Even in the absence of an interaction, task relevance plays an important role \citep{Castelhano2009-px, Henderson2017-it}. These studies point towards gaze control being the consequence of knowledge and task-driven predictions \citep{Henderson2017-tx}.

Moreover, \citet{Belardinelli2016-xb} investigated the role of anticipatory eye movements when interacting with familiar and unfamiliar tools in a controlled lab setting. These tools had differentiable parts: tool handle and effector. The results showed that in the case of unfamiliar tools, preparatory eye movements are made to the tool-effector to extract the mechanical properties of the tool as the semantic information was not readily available. This effect was enhanced when subjects were asked to perform tool-specific movements instead of a generic action of lifting the tool by the handle. The authors, hence, concluded that eye movements are used to actively infer the appropriate usage of the tools from their mechanical properties. In the study, the tools were presented as images on a screen, and participants pantomimed lifting or using the tool. While the study revealed valuable insights into anticipatory gaze control, a question remains if these results are part of natural cognition and can be reproduced in more realistic environments.

\citet{Herbort2011-hf}  further investigated the interaction of habitual and goal-directed processes that affect grasp selection while interacting with everyday objects. They presented objects in different orientations and showed that grasp selection depended on the overarching goal of the movement sequence dependent on the object’s orientation. \citet{Belardinelli2016-kf} further showed that fixations have an anticipatory preference for the region where the index finger is placed. Consequently, the location of fixations is predictive of both proximal goals of manual planning and task-related distal goals.

When studying anticipatory behaviors corresponding to an action, one must also ask whether symbolized action is enough and how real the action should be. \citet{Kroliczak2007-wo} showed brain areas typically involved in real actions are not driven by pantomimed actions and that pantomimed grasps do not activate the object-related regions within the ventral stream. Similarly, \citet{Hermsdorfer2012-ca} showed a weak correlation between the hand trajectories for pantomimed and actual tool interaction. These studies indicate that the realism of sensory and tactile feedback while acting (e.g., a grasp) can be an essential factor when studying anticipatory behavioral control.

In virtual reality (VR), realistic actions can be studied by simulating an interaction within an environment. Using interfaces such as VR controllers, ego-centric visual feedback of a hand can be simulated. These interfaces usually consist of hand-held devices that are tracked in space and through which different actions are controlled by pressing buttons. One advantage of controller-based VR interaction is the possibility of haptic feedback. One disadvantage is that the hand posture while holding the controller does not always correspond to the user’s virtual visual feedback when the simulated hand performs the action. Conversely, camera-based interaction interfaces such as LeapMotion, capture the real-time movements of the user’s hand and use finger gestures, like wrap grasp or pinch grasp, to control different actions in the environment. These interfaces give the user realistic visual feedback of their finer hand and finger movements, while they can not give haptic feedback. Consequently, the chosen method of interaction in VR can afford different levels of realism and could elicit different behavioral responses.

In the present study, we investigated anticipatory gaze control in two different experiments. We were interested in the extent to which the realism of the action affordance and the environment modified the results shown by \citet{Belardinelli2016-xb}. We asked participants to lift or use 3D models of tools in VR that were categorized as familiar or unfamiliar. Additionally, we extended the experimental design to include the tool handle’s spatial orientation, congruent or incongruent to the subjects’ handedness. 

In experiment-I, subjects performed the experiment in a low realism environemnt and action affordance and interacted with the tool models using a VR controller, which mimicked grasp in the virtual environment by pulling the index finger. In experiment-II, subjects were immersed in a high realism setting where they interacted with the tools using LeapMotion, which required natural hand and finger movements. Thus, the action affordance appeared closer to the real world. With this experimental design, we investigate the influence of task, tool familiarity, the spatial orientation of the tool, and, notably, the impact of the realism of the action affordance. 

