\section{Introduction}

A longstanding goal of the cognitive sciences is to understand cognition, behavior, and experience as it unfolds in the natural world \citep{Parada2020-qq}. Given the technological advancements made in the last decade, there are few methodological roadblocks to understanding natural cognition where laboratory studies can be extended to naturalistic settings and hopefully lead towards new insights \citep{Ladouce2016-li, Parada2018-vf}. More recently, a pragmatic turn has emerged in the field where there is a greater push towards incorporating the body and bodily actions to infer cognitive function \citep{Engel2013-bx}. 

Human tool use is an explicitly natural cognitive function that involves the transfer of proximal goals (e.g., placement of grasp) to distal goals for the tool \citep{Arbib2009-wa}.  Moreover, simple tools fundamentally expand the body representations to include representations of the tool in the peripersonal space \citep{Berti2000-ap, Farne2005-ns, Maravita2002-sq}. Furthermore, tool use is differentiated from other object-based actions where the tool is “acted with” \citep{Johnson2003-ee} and requires semantic knowledge of the tool as well as the necessary skill to perform actions with it \citep{Johnson-Frey2004-qk}. Hence, tool use involves complex behaviors ranging from cognitive and semantic reasoning to perceptual and motor processing.

When using tools, a wealth of information is parsed to produce the relevant action. The semantic knowledge associated with the tool helps understand how it is used, the mechanical knowledge maps the physical properties of the tool for potential usage, and finally, sensorimotor knowledge helps decipher possible movements required to use the tool \citep{Baumard2014-kv}. The amalgamation of these knowledge sources (which can be unique to a tool) necessitates planning any action associated with the tool. When this knowledge is not readily available, inferential processes must be deployed to deduce the relevant action. 

\citet{Henderson2017-tx} proposed that gaze control in natural scene can be characterized as the result of knowledge-driven predictions. These predictions for gaze control are based on the knowledge gained from past experiences. Predictive gaze control aims to sample informative and meaningful information in the environment that is relevant to the current needs of the cognitive system. Furthermore, gaze control proactively samples information in anticipation of the following action \citep{Hayhoe2004-ni, Land2001-do, Pelz2001-cn}. \citet{Belardinelli2016-kf}  showed that eye movements are goal-oriented and are modulated in anticipation of the object interaction task. There is strong evidence that task plays a vital role in how the eyes scan the scene and are differentiated between passive viewing and pantomimed interaction \citep{Belardinelli2015-in}. Similarly, \citet{Keshava2020-cp} showed that rudimentary object interactions can be predicted using eye-movement data alone. Even in the absence of an active interaction, task relevance plays an important role \citep{Castelhano2009-px, Henderson2017-it}. These studies point towards gaze control being the consequence of knowledge and task-driven predictions.

In a similar vein, \citet{Belardinelli2016-xb} investigated the role of anticipatory eye movements when interacting with familiar and unfamiliar tools in a controlled lab setting. These tools had differentiable parts: tool handle and effector. The results showed that in the case of unfamiliar tools, preparatory fixations are made on the tool-effector to extract the mechanical properties of the tool as the semantic information was not readily available. This effect was enhanced when subjects were asked to perform tool-specific movements instead of a generic action of lifting the tool by the handle. The authors, hence, concluded that eye movements are used to actively infer the appropriate usage of the tools from their mechanical properties. In the study, the tools were presented as images on a screen, and participants pantomimed lifting or using the tool. While the study revealed valuable insights into anticipatory gaze control, a question remains if these results are part of natural cognition and can be reproduced in more realistic environments.

\citet{Herbort2011-hf}  further investigated the interaction of habitual and goal-directed processes that affect grasp selection while interacting with everyday objects. They presented objects in different orientations and showed that grasp selection depended on the overarching goal of the movement sequence dependent on the object’s orientation. \citet{Belardinelli2016-kf} further showed that fixations have an anticipatory preference for the region where the index finger is placed. Consequently, the location of fixations is predictive of both proximal goals of manual planning and task-related distal goals.

When studying anticipatory behaviors corresponding to an action, there must be a distinction of symbolized or pantomimed vs. real actions. \citet{Kroliczak2007-wo} showed brain areas typically involved in real actions are not driven by pantomimed actions and that pantomimed grasps do not activate the object-related regions within the ventral stream. Similarly, \citet{Hermsdorfer2012-ca} showed a weak correlation between the hand trajectories for pantomimed and actual tool interaction. These studies indicate that the realism of sensory and tactile feedback while acting (e.g., a grasp) can be an essential factor when studying anticipatory behavioral control.

As research steadily moves towards a more ecological view of cognitive processing with bodily actions and interactions with the environment, there is a greater need to understand behavioral differences induced by varying degrees of realism of action affordances. \citet{Chalmers2008-vr} posit that defining levels of realism is necessary to achieve a one-to-one mapping of an experience in the virtual environment with the same experience in the real environment. Moreover, for research purposes such one-to-one mapping is necessary to avoid misrepresenting the real environment. In virtual reality (VR), realistic actions can be produced by different kinds of interaction methods. Using interfaces such as VR controllers, ego-centric visual feedback of a hand can be simulated. These interfaces usually consist of hand-held devices that are tracked in space and through which different actions are controlled by pressing buttons. One advantage of controller-based VR interaction is the possibility of haptic feedback. However, a disadvantage is that the actual hand posture while holding the controller does not necessarily correspond to the user’s simulated hand as they perform the action. Conversely, camera-based interaction interfaces such as LeapMotion, capture the real-time movements of the user’s hand and finger gestures, like wrap grasp or pinch grasp, to control different actions in the environment. These interfaces give the user a realistic simulation of their finer hand and finger movements, while they cannot give direct haptic feedback of the gripped object. Consequently, the chosen method of interaction in VR can afford different levels of realism and could elicit different behavioral responses.

In the present study, we investigated anticipatory gaze control pertaining to tool interactions in two different experiments. We were interested in the extent to which the action affordance and the environment modified active inference processes exhibited in gaze behavior. In experiment-I, subjects performed the experiment in a low realism environment and interacted with the tool models using a VR controller, which mimicked grasp in the virtual environment by pulling the index finger. In experiment-II, subjects were immersed in a high realism setting where they interacted with the tools using LeapMotion, which required natural hand and finger movements. Thus, the action affordance appeared closer to the real-world. Furthermore, in both experiments, participants interacted with 3D models of tools by lifting or using them in VR. For the stimuli set, we used familiar or unfamiliar tools as described in \citet{Belardinelli2016-xb}. Additionally, to differentiate between proximal and distal goal planning, we manipulated the spatial orientation of the tool handle so that they were either congruent or incongruent to the subjects’ handedness.  With this experimental design, we investigate the influence of task, tool familiarity, the spatial orientation of the tool, and, notably, the impact of the action affordance on anticipatory gaze behavior. 

