\section{Discussion}

The primary aim of this study is to investigate how gaze-based strategies vary for a given task, tool familiarity, and tool handle orientation in naturalistic settings and how the action affordance of the environment can affect this gaze behavior. The factors of the task required the production of tool-specific movements in the case of the use task and generic movements in the case of the lift task. We manipulated the factor of tool familiarity, by presenting tools which were either familiar or unfamiliar. We further controlled the tool orientation with the handle oriented to the right or to the left and were congruent and incongruent to the participants' handedness, respectively. Our experiment design differentiated gaze-based planning and the influence of proximal planning related to grasping the tools and the distal goal-oriented planning of acting with the tools. With our study, we successfully added to the current body of research in two important ways. Firstly, irrespective of the action affordance provided by the interaction methods in the virtual environment, the number of fixations and their eccentricity were modulated by distal goal-oriented factors of task and tool familiarity. When cued to use the tools, there were higher odds of fixating on the effector of the tool as compared to lifting the tool. Similarly, when presented with unfamiliar tools, there were higher odds of fixations on the tool effector as compared to familiar tools. This effect was more pronounced when participants were instructed to use an unfamiliar tool. Secondly, only in the case of naturalistic action affordance that allowed for finer hand and finger movements, anticipatory fixations were significantly biased toward the tool handle when it was oriented incongruent to the subjects' handedness. In sum, our study shows that the action affordance of the virtual environment affects the anticipatory fixations related to proximal goal-oriented planning but not distal goal-oriented planning.

As studies of action-oriented behavior in virtual reality get more popular, there is a need to understand how the interaction methods in the virtual environment can affect behavioral outcomes. In our study, we conducted two experiments to disentangle the role of action affordance for goal-oriented planning. While in experiment-I, participants interacted with 3D tool models using VR controllers which produced a virtual grasp by pulling their index fingers, in experiment-II, participants interacted with tool models by producing an actual grasp. Our results show that the action affordance provided in experiment-II greatly biased the fixations towards the tool handle both in number of fixations and the eccentricity of the fixations when the tool handles were incongruent to the subjects' handedness. These results can be interpreted as fixations being biased to plan grasping the tools when the interaction method affords naturalistic movements. This effect is indicative of an end-state comfort planning \citep{Rosenbaum1996-dq, Herbort2012-ma} where preference is given to the final hand posture rather than a comfortable initial posture. While using VR controllers, such planning is not required as the hand posture is largely fixed. In line with \citet{Pezzulo_undated-gx}, our study shows that anticipatory eye movements can reveal motor planning that selects appropriate movements to achieve the current goal.  

Furthermore, the different interaction methods did not affect distal goal-oriented planning. In both experiments, the odds of sampling visual information from the mechanical properties of a tool are different based on the specificity of the task. Moreover, given tool familiarity, the odds of fixating on the tool effector increased for unfamiliar tools. This effect was more pronounced when subjects were instructed to produce tool-specific movements for unfamiliar tools. These results pertaining to distal goal-oriented planning are in line with the findings reported by \citet{Belardinelli2016-xb}. In our study, we show that well before action initiation, subjects attended to the relevant tool parts to produce the tool-specific actions irrespective of the hand posture afforded by the interaction method.

VR has been poised as a viable method to probe cognitive processing in ecologically valid settings without sacrificing experimental control \citep{Parsons2015-eo}. In our study, we probe how the action affordance of the interaction methods in VR affect planning behavior. Our findings give a fuller view of planning strategies that are needed to produce relevant action. Our study shows that irrespective of the interaction method, semantic and sensorimotor knowledge are inferred from the mechanical properties of the tools as revealed by anticipatory fixations. However, naturalistic actions afforded by the interaction method can bias the anticipatory fixations towards the proximal goals of grasping the tool. Specifically, multiple factors such as semantic and sensorimotor knowledge coupled with end-state comfort planning contribute to anticipatory goal planning. Notably, our study shows that different constraints on the interaction method can also result in different anticipatory behavioral responses. From the perspective of \citet{Gibson1977-zj}, the affordances of the environment are tightly linked to the actions that one can perform in it. Similarly, \citet{ORegan2001-pd} posited that actions constitute the cognitive processes that govern relevant sensorimotor contingencies. Hence, our study offers a veridical and ecological valid context to aspects of anticipatory behavior control.

Studies in eye-hand coordination \citep{Johansson2001-sa, Lohmann2019-fy, Belardinelli2018-xm} have shown that eye movements are proactively made towards the grasp contact points. Furthermore, \citet{Flanagan2006-ql} proposed that predictions are made in an event-oriented manner and are at the heart of successful control strategies for object manipulations. They posit that predicted sensory events are compared with actual events like grasping, lifting, moving the object to monitor task progression. With our study, we make the case that gaze-based predictions are made for action outcomes at different scales, and that eye movements are used to plan both proximal goal of grasping the tools and the distal task-specific requirements well before action initiation. 

Our study adds to the growing body of evidence that anticipation and prediction are at the core of cognition \citep{Pezzulo2007-ot}. Motor theories of cognition have proposed that simulations of actions reuse internal models of motor commands to effect multiple predictions \citep{Jeannerod2006-dt}. The simulation of action theory has been used to explain numerous phenomena of planning, prediction of external events, visual perception, and imitation. \citet{Hoffmann2003-ur} introduced anticipatory behavior control as the mechanism by which action-effect representations are activated by the need for an effect-related goal and contingent stimuli. Furthermore, \citet{Pezzulo2021-te} recently proposed that generative models provide top-down predictive signals for perception, cognition, and action during active tasks and these signals are otherwise weak and/or absent when the brain is at rest or the stimuli are weak. Our study shows that anticipatory behavior is tightly linked to the production of task-relevant actions and is contextualized to the action affordance of the environment.

We conducted the present study in virtual reality, which is still a burgeoning technology for vision research. While VR environments pose an exciting avenue of research, there are still limitations that practitioners must face while conducting experiments. First, the naturalistic setting of both experiments I and II afforded natural head movements. To maintain optimal quality over the data, we asked the participants in the study to make limited head movements. Additionally, we presented the tools and the task cues not to cause extreme pitch head movements. Secondly, mobile eye-trackers can be error-prone and might suffer from variable sampling rates \citep{Ehinger2019-xr} or calibration errors due to slippage \citep{Niehorster2020-yq}. To mitigate any calibration errors, we also made sure that we calibrated the eye-trackers at regular intervals. Thirdly, both controller-based and camera-based VR interaction methods are still new technology. It could have been challenging for participants to get used to, even though we made sure they practiced the interaction before the experiment. While we simulated grasping the tool using LeapMotionâ€™s gesture recognition and were able to produce a more realistic actions, it is still an inadequate substitute for a real grasp where the tactile feedback of the tool in hand might elicit more accurate responses. For example,\citet{Ozana2018-ih} showed that grasping movements within a virtual environment differ both quantitatively and qualitatively from typical grasping. Lastly, while there are visible differences between the environments, we see that there are no significant differences between the percentage of fixations allocated to the tool and the rest of the environment in both the experimental settings. Hence, we contend that the differences in the eye movement behavior reported in the study are largely a consequence of the differences in the action affordance and much less because of mere visual differences of the environments. In light of these limitations, we know that our study must be considered from a nuanced perspective. Furthermore, there is still room for replicating our study with novel and more realistic interaction methods.

There are still some open questions pertaining to anticipatory behavior elicited by tool interactions. Firstly, while our study distinguishes between levels of action affordances, future work can look at goal-oriented planning for passive observers at both proximal and distal levels. Secondly, it would be interesting to dive deeper into the predictive neural signals that give rise to the present oculomotor behaviors. Our study provides a first step towards distinctly investigating proximal and distal goal-oriented planning.

\section{Conclusion}

The present study gives a veridical and ecologically valid context to planning and anticipatory gaze behavior. Our results support the hypothesis that eye movements serve the cognitive function of actively sampling information from the environment to produce relevant actions. When semantic information about the object is not readily available, eye movements are used to seek information from its mechanical properties from specific locations. Furthermore, we show that fixations are made in a goal-oriented way in anticipation of the relevant action. When considering the realism of the action affordance, our results show that eye movements are affected by both proximal goals of manually grasping objects and the distal task-based demands. Lastly, our study is at the frontiers of naturalistic vision research, where novel technologies can be harnessed to answer questions that were previously far-fetched.


